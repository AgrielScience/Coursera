{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T14:43:00.534866Z",
     "start_time": "2018-04-11T14:43:00.385256Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 70)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import seaborn as sns\n",
    "sns.set_style(style='whitegrid')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from datetime import datetime, date\n",
    "from itertools import product\n",
    "import re\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gc\n",
    "from tqdm import tqdm_notebook\n",
    "import pickle\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:18:48.229171Z",
     "start_time": "2018-04-11T13:18:48.216930Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols = [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols] = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:23:11.180731Z",
     "start_time": "2018-04-11T13:23:07.415988Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sales = pd.read_csv('data/sales_train.csv', index_col=[0])\n",
    "\n",
    "test = pd.read_csv('data/test.csv', index_col=[0])\n",
    "\n",
    "item_categories = pd.read_csv('data/item_categories.csv', index_col=[0])\n",
    "\n",
    "items = pd.read_csv('data/items.csv', index_col=[0])\n",
    "\n",
    "shops = pd.read_csv('data/shops.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.xlim(-100, 3000)\n",
    "sns.boxplot(x=sales.item_cnt_day)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.xlim(sales.item_price.min(), sales.item_price.max()*1.1)\n",
    "sns.boxplot(x=sales.item_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sales[sales.item_price<100000]\n",
    "sales = sales[sales.item_cnt_day<1001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\n",
    "shops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\n",
    "shops['city_code'] = LabelEncoder().fit_transform(shops['city'])\n",
    "\n",
    "sales = pd.merge(sales,shops[['shop_id', 'city']], on = 'shop_id')\n",
    "test = pd.merge(test, shops[['shop_id', 'city']], on = 'shop_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add item_category_id to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:23:12.750496Z",
     "start_time": "2018-04-11T13:23:12.004636Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_categories['category_name'] = item_categories.index\n",
    "items['name'] = items.index\n",
    "items = pd.merge(items,item_categories, on = 'item_category_id')\n",
    "\n",
    "sales = pd.merge(sales, items[['item_category_id', 'item_id']], on='item_id')\n",
    "test = pd.merge(test, items[['item_category_id', 'item_id']], on='item_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticed, what in test dataset 42 unique shop_ids and 5100 item_ids, their multiplication equals lenght of test dataset (214200). That means what for test dataset were took all existing the pairs of shop and item. There are also 363 new unique items, which are not in train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also several shops, which have different names in dataset, but i think, what thea are the same:\n",
    "<br>\n",
    "'!Якутск Орджоникидзе, 56 фран' == 'Якутск Орджоникидзе, 56'\n",
    "<br>\n",
    "'!Якутск ТЦ \"Центральный\" фран' == 'Якутск ТЦ \"Центральный\"'\n",
    "<br>\n",
    "'Жуковский ул. Чкалова 39м²' == 'Жуковский ул. Чкалова 39м?'\n",
    "<br>\n",
    "So, change their ids in train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:27:28.105557Z",
     "start_time": "2018-04-11T13:27:27.627402Z"
    }
   },
   "outputs": [],
   "source": [
    "sales.loc[sales[sales.shop_id==0].index, 'shop_id'] = 57\n",
    "sales.loc[sales[sales.shop_id==1].index, 'shop_id'] = 58\n",
    "sales.loc[sales[sales.shop_id==11].index, 'shop_id'] = 10\n",
    "\n",
    "test[test.shop_id==0].shop_id=57\n",
    "test[test.shop_id==1].shop_id=58\n",
    "test[test.shop_id==11].shop_id=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare features. Make many aggregations by shop-item-month, shop-month, item-month, city-month, category-month, city-item-month, city-category-month and shop-category-month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "grid = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "\n",
    "# Turn the grid into a dataframe\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "\n",
    "# Groupby data to get shop-item-month aggregates\n",
    "gb = sales.groupby(['shop_id', 'item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n",
    "# Fix column names\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n",
    "# Join it to the grid\n",
    "all_data = pd.merge(grid, gb, how='left', on=['shop_id', 'item_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "all_data = pd.merge(all_data, shops[['city', 'shop_id']],\n",
    "                on='shop_id', how='left')\n",
    "\n",
    "all_data = pd.merge(all_data, items[['item_category_id', 'item_id']],\n",
    "                on='item_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but with shop-month aggregates\n",
    "gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum'}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Same as above but with item-month aggregates\n",
    "gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum'}})\n",
    "gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Same as above but with city-month aggregates\n",
    "gb = sales.groupby(['city', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_city':'sum'}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['city', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Same as above but with category-month aggregates\n",
    "gb = sales.groupby(['item_category_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_category':'sum'}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_category_id', 'date_block_num']).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:36:53.576088Z",
     "start_time": "2018-04-11T13:36:53.573008Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Groupby data to get city-item-month aggregates\n",
    "gb = sales.groupby(['city', 'item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_city_item':'sum'}})\n",
    "# Fix column names\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n",
    "# Join it to the grid\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['city', 'item_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Groupby data to get city-category-month aggregates\n",
    "gb = sales.groupby(['city', 'item_category_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_city_category':'sum'}})\n",
    "# Fix column names\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n",
    "# Join it to the grid\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['city', 'item_category_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Groupby data to get shop-category-month aggregates\n",
    "gb = sales.groupby(['shop_id', 'item_category_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop_category':'sum'}})\n",
    "# Fix column names\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n",
    "# Join it to the grid\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'item_category_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Downcast dtypes from 64 to 32 bit to save memory\n",
    "all_data = downcast_dtypes(all_data)\n",
    "del grid, gb\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatinate test dataset with 'all_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['date_block_num']=34\n",
    "all_data = pd.concat((all_data,test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add some lag features for all aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "cols_to_rename = list(all_data.columns.difference(index_cols).difference(['city','item_category_id']))\n",
    "\n",
    "shift_range = [1, 2, 3, 4, 5, 6, 12]\n",
    "\n",
    "for month_shift in tqdm_notebook(shift_range):\n",
    "    train_shift = all_data[index_cols + cols_to_rename + ['city','item_category_id']].copy()\n",
    "    \n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "    \n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "    all_data = pd.merge(all_data, train_shift[index_cols+[f\"target_lag_{month_shift}\"]], on=index_cols, how='left').fillna(0)\n",
    "    \n",
    "    all_data = pd.merge(all_data, train_shift[['date_block_num','shop_id', f\"target_shop_lag_{month_shift}\"]].drop_duplicates().dropna(), on=['date_block_num','shop_id'],\n",
    "                        how='left').fillna(0)\n",
    "    all_data = pd.merge(all_data, train_shift[['date_block_num','item_id', f\"target_item_lag_{month_shift}\"]].drop_duplicates().dropna(), on=['date_block_num','item_id'],\n",
    "                        how='left').fillna(0)\n",
    "    all_data = pd.merge(all_data, train_shift[['date_block_num','item_category_id', f\"target_category_lag_{month_shift}\"]].drop_duplicates().dropna(), on=['date_block_num','item_category_id'],\n",
    "                        how='left').fillna(0)\n",
    "    all_data = pd.merge(all_data, train_shift[['date_block_num','city', f\"target_city_lag_{month_shift}\"]].drop_duplicates().dropna(), on=['date_block_num','city'],\n",
    "                        how='left').fillna(0)\n",
    "    all_data = pd.merge(all_data, train_shift[['date_block_num','city', 'item_category_id', f\"target_city_category_lag_{month_shift}\"]].drop_duplicates().dropna(),\n",
    "                        on=['date_block_num','city','item_category_id'], how='left').fillna(0)\n",
    "    all_data = pd.merge(all_data, train_shift[['date_block_num','city', 'item_id', f\"target_city_item_lag_{month_shift}\"]].drop_duplicates().dropna(),\n",
    "                        on=['date_block_num','city','item_id'], how='left').fillna(0)\n",
    "    all_data = pd.merge(all_data, train_shift[['date_block_num','shop_id', 'item_category_id', f\"target_shop_category_lag_{month_shift}\"]].drop_duplicates().dropna(),\n",
    "                        on=['date_block_num','shop_id','item_category_id'], how='left').fillna(0)\n",
    "del train_shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:38:32.683004Z",
     "start_time": "2018-04-11T13:38:32.679763Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Don't use old data from year 2013\n",
    "all_data = all_data[all_data['date_block_num'] >= 12]\n",
    "\n",
    "# List of all lagged features\n",
    "fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n",
    "# We will drop these at fitting stage\n",
    "to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n",
    "\n",
    "# Category for each item\n",
    "#item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n",
    "\n",
    "#all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n",
    "all_data = downcast_dtypes(all_data)\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add month, city and name of shop to dataset and make dummy city features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T13:44:12.079946Z",
     "start_time": "2018-04-11T13:44:12.055147Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data['month'] = all_data.date_block_num%12\n",
    "\n",
    "all_data = pd.merge(all_data, pd.get_dummies(all_data.city), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make text features from item names and item category names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T14:13:45.834119Z",
     "start_time": "2018-04-11T14:13:45.782406Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_proc(x):\n",
    "    # clear text\n",
    "    text = re.sub(r'[\\W_]+', u' ', x.lower(), flags=re.UNICODE)\n",
    "    text = re.sub(u'\\d', u' ',text, flags=re.UNICODE).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data, shops[['shop_id', 'shop_name']], on = 'shop_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T14:17:43.745948Z",
     "start_time": "2018-04-11T14:17:43.741304Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# item names\n",
    "all_data.shop_name = all_data.shop_name.apply(text_proc)\n",
    "\n",
    "feature_cnt = 30\n",
    "tfidf = TfidfVectorizer(max_features=feature_cnt)\n",
    "txtFeatures = pd.DataFrame(tfidf.fit_transform(all_data['shop_name']).toarray())\n",
    "cols = txtFeatures.columns\n",
    "for i in range(feature_cnt):\n",
    "    all_data['item_name_tfidf_' + str(i)] = txtFeatures[cols[i]]\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T14:18:04.603157Z",
     "start_time": "2018-04-11T14:18:04.599665Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# item category names\n",
    "all_data = pd.merge(all_data,item_categories[['item_category_id', 'category_name']], on = 'item_category_id', how = 'left')\n",
    "\n",
    "all_data.category_name = all_data.category_name.apply(text_proc)\n",
    "feature_cnt = 15\n",
    "tfidf = TfidfVectorizer(max_features=feature_cnt)\n",
    "txtFeatures = pd.DataFrame(tfidf.fit_transform(all_data['category_name']).toarray())\n",
    "cols = txtFeatures.columns\n",
    "for i in range(feature_cnt):\n",
    "    all_data['category_name_tfidf_' + str(i)] = txtFeatures[cols[i]]\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add additional features of length of item names and category names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data['item_name_len'] = all_data['shop_name'].map(len)\n",
    "all_data['item_name_wc'] = all_data['shop_name'].map(lambda x: len(str(x).split(' ')))\n",
    "\n",
    "all_data['category_name_len'] = all_data['category_name'].map(len)\n",
    "all_data['category_name_wc'] = all_data['category_name'].map(lambda x: len(str(x).split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T14:34:20.650391Z",
     "start_time": "2018-04-11T14:34:19.848126Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_data target distribution\n",
    "sns.kdeplot(all_data.target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print several random chosen time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T14:42:02.981566Z",
     "start_time": "2018-04-11T14:42:01.583769Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "pairs = all_data[['item_id','shop_id']].drop_duplicates()\n",
    "for i in range(10):\n",
    "    rand = np.random.randint(0,len(pairs))\n",
    "    all_data[(all_data.shop_id==pairs.iloc[rand].shop_id)&(all_data.item_id==pairs.iloc[rand].item_id)].target.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv('dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop_cols.append('shop_name')\n",
    "to_drop_cols.append('category_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T14:42:40.676309Z",
     "start_time": "2018-04-11T14:42:40.672563Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = all_data['date_block_num']\n",
    "\n",
    "dates_train = dates[dates <  33]\n",
    "dates_val = dates[dates ==  33]\n",
    "dates_test  = dates[dates == 34]\n",
    "\n",
    "X_train = all_data.loc[dates <  33].drop(to_drop_cols, axis=1)\n",
    "X_val =  all_data.loc[dates == 33].drop(to_drop_cols, axis=1)\n",
    "X_test =  all_data.loc[dates == 34].drop(to_drop_cols, axis=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit_transform(X_train)\n",
    "scaler.transform(X_val)\n",
    "scaler.transform(X_test)\n",
    "\n",
    "y_train = all_data.loc[dates <  33, 'target'].values\n",
    "y_val =  all_data.loc[dates == 33, 'target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T14:46:31.883122Z",
     "start_time": "2018-04-11T14:46:31.879803Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "\n",
    "model = XGBRegressor(\n",
    "    max_depth=8,\n",
    "    n_estimators=1000,\n",
    "    min_child_weight=300, \n",
    "    colsample_bytree=0.8, \n",
    "    subsample=0.8, \n",
    "    eta=0.3,    \n",
    "    seed=42)\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    eval_metric=\"rmse\", \n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "    verbose=True, \n",
    "    early_stopping_rounds = 10)\n",
    "\n",
    "time.time() - ts\n",
    "\n",
    "xgb_val_pred = model.predict(X_val)\n",
    "xgb_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T14:46:39.332019Z",
     "start_time": "2018-04-11T14:46:39.328609Z"
    }
   },
   "outputs": [],
   "source": [
    "lgbmodel=LGBMRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=32,\n",
    "        colsample_bytree=0.9497036,\n",
    "        subsample=0.8715623,\n",
    "        max_depth=8,\n",
    "        reg_alpha=0.04,\n",
    "        reg_lambda=0.073,\n",
    "        min_split_gain=0.0222415,\n",
    "        min_child_weight=40)\n",
    "\n",
    "lgbmodel.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "             early_stopping_rounds=10, eval_metric=\"l2_root\")\n",
    "\n",
    "lgb_val_pred = model.predict(X_val)\n",
    "lgb_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T14:46:47.052334Z",
     "start_time": "2018-04-11T14:46:47.049135Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=50, max_depth=7, random_state=0, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "rf_val_pred = rf_model.predict(X_val)\n",
    "rf_test_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegression(n_jobs=-1)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "lr_val_pred = lr_model.predict(X_val)\n",
    "lr_test_pred = lr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsRegressor(n_neighbors=9, leaf_size=13, n_jobs=-1)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "knn_val_pred = knn_model.predict(X_val)\n",
    "knn_test_pred = knn_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensembling of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T14:47:48.789759Z",
     "start_time": "2018-04-11T14:47:48.786407Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_level = pd.DataFrame(lgb_val_pred, columns=['lgbm'])\n",
    "first_level['xgbm'] = xgb_val_pred\n",
    "first_level['random_forest'] = rf_val_pred\n",
    "first_level['linear_regression'] = lr_val_pred\n",
    "first_level['knn'] = knn_val_pred\n",
    "first_level['label'] = y_val\n",
    "\n",
    "first_level_test = pd.DataFrame(lgb_test_pred, columns=['lgbm'])\n",
    "first_level_test['xgbm'] = xgb_test_pred\n",
    "first_level_test['random_forest'] = rf_test_pred\n",
    "first_level_test['linear_regression'] = lr_test_pred\n",
    "first_level_test['knn'] = knn_test_pred\n",
    "first_level_test.head(5)\n",
    "\n",
    "meta_model = LinearRegression(n_jobs=-1)\n",
    "first_level.drop('label', axis=1, inplace=True)\n",
    "meta_model.fit(first_level, y_val)\n",
    "\n",
    "ensemble_pred = meta_model.predict(first_level)\n",
    "final_predictions = meta_model.predict(first_level_test)\n",
    "\n",
    "prediction_df = pd.DataFrame(test['ID'], columns=['ID'])\n",
    "prediction_df['item_cnt_month'] = final_predictions.clip(0., 20.)\n",
    "prediction_df.to_csv('submission.csv', index=False)\n",
    "prediction_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
